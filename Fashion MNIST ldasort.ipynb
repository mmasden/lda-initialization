{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ldasort.ldasort' from '/home/kbahnsen/Development/Marissa/ldasort/ldasort/ldasort.py'>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Custom Tools\n",
    "import ldasort.ldasort as ldasort\n",
    "\n",
    "## Import Standard Data Processing Tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#import ML tools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from importlib import reload\n",
    "reload(ldasort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD Fashion MNIST\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "data_set = \"FashionMNIST\" \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "#normalize\n",
    "x_train_normalized = (x_train - np.mean(x_train))/np.std(x_train) \n",
    "x_test_normalized = (x_test - np.mean(x_train))/np.std(x_train)\n",
    "\n",
    "#flatten and reshape\n",
    "x_train_normalized = x_train_normalized.reshape(60000,28*28)\n",
    "x_test_normalized = x_test_normalized.reshape(10000,28*28)\n",
    "y_train=np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 sorted\n",
      "Label 1 sorted\n",
      "Label 2 sorted\n",
      "Label 3 sorted\n",
      "Label 4 sorted\n",
      "Label 5 sorted\n",
      "Label 6 sorted\n",
      "Label 7 sorted\n",
      "Label 8 sorted\n",
      "Label 9 sorted\n"
     ]
    }
   ],
   "source": [
    "# Find weights and biases using LDA sorting algorithm. \n",
    "# Comment out if weights already present. \n",
    "\n",
    "weights, biases = ldasort.ldasort(x_train_normalized, y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights and biases for use later. \n",
    "\n",
    "np.savez(\"Sorted_Weights_{}\".format(data_set), weights=weights, biases=biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to run trials \n",
    "\n",
    "file = np.load(\"Sorted_Weights_{}.npz\".format(data_set))\n",
    "\n",
    "weights = file[\"weights\"]\n",
    "biases = file[\"biases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial with Batch Size 25 Learning Rate 0.001 Done, time elapsed 2.0 minutes\n",
      "Trial with Batch Size 25 Learning Rate 0.005 Done, time elapsed 4.0 minutes\n",
      "Trial with Batch Size 25 Learning Rate 0.01 Done, time elapsed 6.0 minutes\n",
      "Trial with Batch Size 100 Learning Rate 0.001 Done, time elapsed 7.0 minutes\n",
      "Trial with Batch Size 100 Learning Rate 0.005 Done, time elapsed 7.0 minutes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-65230141a634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             history=model.fit(x_train_normalized, \n\u001b[0m\u001b[1;32m     76\u001b[0m                       \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### create loop for comparing across batch sizes, learning rates\n",
    "\n",
    "learning_rates = [.001, .005, .01]\n",
    "batch_sizes = [25, 100, 500]\n",
    "\n",
    "#small values for quick sample run. For full length runs, use n_trials=20, n_epochs=100\n",
    "n_trials = 5\n",
    "n_epochs = 5\n",
    "\n",
    "t0=timer()\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "\n",
    "        temp_hists=[]\n",
    "        val_hists=[]\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "\n",
    "            #set up LDA-initialized model\n",
    "            \n",
    "            model=ldasort.setup(intermediate_neurons=len(weights),input_shape=784, output_shape=10)\n",
    "\n",
    "            model.compile(\n",
    "                    optimizer=keras.optimizers.SGD(learning_rate=lr),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    weighted_metrics=['accuracy']\n",
    "                    )\n",
    "\n",
    "            model.layers[0].set_weights([weights.T,biases])\n",
    "        \n",
    "\n",
    "            history=model.fit(x_train_normalized, \n",
    "                      y_train, \n",
    "                      epochs=n_epochs, \n",
    "                      batch_size=batch_size,\n",
    "                      validation_data = (x_test_normalized, y_test),\n",
    "                      verbose=False)\n",
    "\n",
    "            temp_hists.append(history.history['accuracy'])\n",
    "            val_hists.append(history.history[\"val_accuracy\"])\n",
    "\n",
    "\n",
    "            keras.backend.clear_session()\n",
    "\n",
    "        lda_hist = temp_hists\n",
    "        val_lda_hist=val_hists\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        #Run Trials with Random Initialization#\n",
    "        #######################################\n",
    "        temp_hists=[]\n",
    "        val_hists=[]\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "\n",
    "            #set up randomly-initialized model\n",
    "            \n",
    "            model=ldasort.setup(intermediate_neurons=len(weights),input_shape=784, output_shape=10)\n",
    "\n",
    "            model.compile(\n",
    "                    optimizer=keras.optimizers.SGD(learning_rate=lr),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    weighted_metrics=['accuracy']\n",
    "                    )\n",
    "            \n",
    "            #do not set weights\n",
    "            #model.layers[0].set_weights([weights.T,biases])\n",
    "            \n",
    "\n",
    "\n",
    "            history=model.fit(x_train_normalized, \n",
    "                      y_train, \n",
    "                      epochs=n_epochs, \n",
    "                      batch_size=batch_size,\n",
    "                      validation_data = (x_test_normalized, y_test),\n",
    "                      verbose=False)\n",
    "\n",
    "            temp_hists.append(history.history['accuracy'])\n",
    "            val_hists.append(history.history[\"val_accuracy\"])\n",
    "            \n",
    "\n",
    "            keras.backend.clear_session()\n",
    "\n",
    "        rand_hist = temp_hists\n",
    "        val_rand_hist=val_hists\n",
    "        \n",
    "\n",
    "        np.savez(\"{}_sigmoid_lr{}_bat{}\".format(data_set, lr,batch_size), \n",
    "                 lda_acc=lda_hist, \n",
    "                 val_lda_acc=val_lda_hist,\n",
    "                 rand_acc = rand_hist, \n",
    "                 val_rand_acc = val_rand_hist,\n",
    "                 n_components=len(weights), \n",
    "                num_trials=n_trials,\n",
    "                n_epochs = n_epochs)\n",
    "        \n",
    "        \n",
    "        print(\"Trial with Batch Size {} Learning Rate {} Done, time elapsed {} minutes\".format(\n",
    "            batch_size, lr, np.round((timer()-t0)/60),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct plots. \n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "\n",
    "data = dict()\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        data[(batch_size, lr)]=np.load(\"{}_sigmoid_lr{}_bat{}.npz\".format(data_set, lr, batch_size))\n",
    "        \n",
    "        fig,ax=plt.subplots()\n",
    "        \n",
    "\n",
    "        for randplot,ldaplot in zip(data[(batch_size, lr)][\"val_rand_acc\"],data[(batch_size, lr)][\"val_lda_acc\"]): #j is random h is preset \n",
    "            ax.plot(ldaplot,c='red',alpha=.1)\n",
    "            ax.plot(randplot,c='blue',alpha=.1)\n",
    "\n",
    "\n",
    "        #ax.plot([threshold]*nepochs,c='k',alpha=.2)\n",
    "        ax.set_ylim((0,1))\n",
    "        ax.set_title(\"FashionMNIST Sigmoid \\n lr {}, batch size {}\".format(lr, batch_size))      \n",
    "        ax.legend([\"LDA Sorted\",\"Random\"])\n",
    "        plt.savefig(\"imgs/fashionmnist_lr{}_bat{}.png\".format(lr,batch_size))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
